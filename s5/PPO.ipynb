{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangping/anaconda3/envs/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Fix missing padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS as padding token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "old_model = AutoModelForCausalLM.from_pretrained(model_name)  # Reference model (Î¸_old)\n",
    "\n",
    "# Resize embeddings after adding new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "old_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RLHF dataset\n",
    "# https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets\n",
    "dataset = load_dataset(\"yitingxie/rlhf-reward-datasets\")\n",
    "\n",
    "# Use a subset for fast training\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))  # Reduce size for demo\n",
    "test_dataset = dataset[\"train\"].shuffle(seed=42).select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO requires a reward model that evaluates responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model  # Base LM acts as feature extractor\n",
    "        self.reward_head = nn.Linear(model.config.hidden_size, 1)  # Reward prediction head\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]  # Use last layer representations\n",
    "        reward = self.reward_head(last_hidden_state[:, -1, :])  # Predict reward from last token\n",
    "        return reward.squeeze(-1)  # Ensure scalar output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use stable-baselines3 PPO with a custom reward function based on human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class PPOEnv(gym.Env):\n",
    "    \"\"\"Custom PPO environment for RLHF training.\"\"\"\n",
    "    def __init__(self, dataset, tokenizer, model, reward_model):\n",
    "        super(PPOEnv, self).__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.reward_model = reward_model\n",
    "        self.current_index = 0  # Track dataset index\n",
    "\n",
    "        # Define observation space (prompt as text tokens)\n",
    "        self.observation_space = spaces.Box(low=0, high=tokenizer.vocab_size, shape=(512,), dtype=np.int32)\n",
    "\n",
    "        # Define action space (select next tokens)\n",
    "        self.action_space = spaces.Discrete(tokenizer.vocab_size)  \n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment with a new prompt and return tokenized observation.\"\"\"\n",
    "        self.current_index = (self.current_index + 1) % len(self.dataset)\n",
    "        prompt = self.dataset[self.current_index][\"prompt\"]\n",
    "\n",
    "        # Tokenize prompt and return as observation\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=512)[\"input_ids\"][0]\n",
    "        return input_ids  # The observation (numerical tokenized prompt)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step: generate a response and compute reward.\"\"\"\n",
    "        prompt = self.dataset[self.current_index][\"prompt\"]\n",
    "\n",
    "        # Convert action (token IDs) back to text\n",
    "        response = self.tokenizer.decode([action], skip_special_tokens=True)\n",
    "\n",
    "        # Tokenize prompt + response\n",
    "        inputs = self.tokenizer(prompt, response, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Compute reward from reward model\n",
    "        with torch.no_grad():\n",
    "            reward = self.reward_model(inputs.input_ids, inputs.attention_mask).item()\n",
    "\n",
    "        # PPO expects (observation, reward, done, info)\n",
    "        done = True  # Single-step episode\n",
    "        return self.reset(), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangping/anaconda3/envs/rlhf/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/fangping/anaconda3/envs/rlhf/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reward_model = RewardModel(model).to(device)\n",
    "\n",
    "# Wrap environment properly\n",
    "ppo_env = DummyVecEnv([lambda: PPOEnv(train_dataset, tokenizer, model, reward_model)])\n",
    "\n",
    "# Initialize PPO model\n",
    "ppo_model = PPO(\"MlpPolicy\", ppo_env, verbose=1, learning_rate=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 120  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 17   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 118           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6208989e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -10.8         |\n",
      "|    explained_variance   | -0.257        |\n",
      "|    learning_rate        | 5e-06         |\n",
      "|    loss                 | 8.89          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00313      |\n",
      "|    value_loss           | 18.6          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 117           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1865896e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -10.8         |\n",
      "|    explained_variance   | -0.253        |\n",
      "|    learning_rate        | 5e-06         |\n",
      "|    loss                 | 7.02          |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00535      |\n",
      "|    value_loss           | 16            |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 117           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 69            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.3852094e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -10.8         |\n",
      "|    explained_variance   | -0.261        |\n",
      "|    learning_rate        | 5e-06         |\n",
      "|    loss                 | 6.94          |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00619      |\n",
      "|    value_loss           | 13.6          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 116           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 87            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010497912 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -10.8         |\n",
      "|    explained_variance   | -0.261        |\n",
      "|    learning_rate        | 5e-06         |\n",
      "|    loss                 | 4.59          |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00704      |\n",
      "|    value_loss           | 11.2          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f742fba9570>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train PPO model on RLHF dataset\n",
    "ppo_model.learn(total_timesteps=10000)  # Increase for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "Human: How do contacts improve your vision?\n",
      "\n",
      "Assistant: They help you see more clearly.\n",
      "\n",
      "Human: Yes, I know. What is the technical or scientific answer for how contacts work to improve vision?\n",
      "\n",
      "Assistant: They increase the amount of light coming into the eye.\n",
      "\n",
      "Human: Are contacts better than wearing glasses?\n",
      "\n",
      "Assistant: Many people prefer contacts.  Some people like the option of switching back and forth between contacts and glasses.  Some people wear glasses for reading or work and use contacts for everything else.  Others wear contacts most of the time, but need glasses for certain activities, like sports or computer work.\n",
      "\n",
      "Human: Are there any risks for using contact lenses?\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Human: How do contacts improve your vision?\n",
      "\n",
      "Assistant: They help you see more clearly.\n",
      "\n",
      "Human: Yes, I know. What is the technical or scientific answer for how contacts work to improve vision?\n",
      "\n",
      "Assistant: They increase the amount of light coming into the eye.\n",
      "\n",
      "Human: Are contacts better than wearing glasses?\n",
      "\n",
      "Assistant: Many people prefer contacts.  Some people like the option of switching back and forth between contacts and glasses.  Some people wear glasses for reading or work and use contacts for everything else.  Others wear contacts most of the time, but need glasses for certain activities, like sports or computer work.\n",
      "\n",
      "Human: Are there any risks for using contact lenses?\n",
      "\n",
      "\n",
      "Assistant: There are some risks.  Some people have trouble with contact lenses because they are too small.  Some people have trouble with contact lenses because they are too large.  Some people have trouble with contact lenses because they are too small.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "Human: How do I make a honey baked ham at home for Christmas dinner?\n",
      "\n",
      "Assistant: You can make honey baked ham by soaking a bone-in ham in a mixture of honey, brown sugar, garlic, and apple cider vinegar.  Make sure the bone side is completely covered in the mixture, then place the ham in a baking dish.  Drizzle with more honey, brown sugar, and vinegar mixture.  Bake for an hour at 325Â°F, or until cooked through.  \n",
      "You can also make a sweet glaze to pour over the ham during the final 15 minutes of cooking.  Start with 1/2 cup each of honey and brown sugar, add a few tablespoons of apple cider vinegar and some garlic and/or onion.  If\n",
      "\n",
      "Human: wow thats a great recipe. do you have any variations that are sugar free?\n",
      "\n",
      "Assistant: I recommend using agave nectar in place of the brown sugar.  And you can try vegetables like spinach or tomatoes as a side.\n",
      "\n",
      "Human: agave nectar is a great suggestion. I will do that. should I use pineapple and/or cherries on the ham as well?\n",
      "\n",
      "Assistant: Try a mixture of chopped pineapple and cherries if you want something sweet and fruity.  The cherries should go on after baking for the final 15 minutes of cooking.\n",
      "\n",
      "Human: wow thats great advice - I think I will try that. what sides go with ham made in this fashion?\n",
      "\n",
      "Assistant: Potatoes and vegetables are a good pairing for ham.  Try roasted potatoes, or a green vegetable side like broccoli or asparagus.  If you want something more fruit-based, try roasted sweet potatoes or a salad.\n",
      "\n",
      "Human: thanks - you have helped me with my dinner plans!\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Human: How do I make a honey baked ham at home for Christmas dinner?\n",
      "\n",
      "Assistant: You can make honey baked ham by soaking a bone-in ham in a mixture of honey, brown sugar, garlic, and apple cider vinegar.  Make sure the bone side is completely covered in the mixture, then place the ham in a baking dish.  Drizzle with more honey, brown sugar, and vinegar mixture.  Bake for an hour at 325Â°F, or until cooked through.  \n",
      "You can also make a sweet glaze to pour over the ham during the final 15 minutes of cooking.  Start with 1/2 cup each of honey and brown sugar, add a few tablespoons of apple cider vinegar and some garlic and/or onion.  If\n",
      "\n",
      "Human: wow thats a great recipe. do you have any variations that are sugar free?\n",
      "\n",
      "Assistant: I recommend using agave nectar in place of the brown sugar.  And you can try vegetables like spinach or tomatoes as a side.\n",
      "\n",
      "Human: agave nectar is a great suggestion. I will do that. should I use pineapple and/or cherries on the ham as well?\n",
      "\n",
      "Assistant: Try a mixture of chopped pineapple and cherries if you want something sweet and fruity.  The cherries should go on after baking for the final 15 minutes of cooking.\n",
      "\n",
      "Human: wow thats great advice - I think I will try that. what sides go with ham made in this fashion?\n",
      "\n",
      "Assistant: Potatoes and vegetables are a good pairing for ham.  Try roasted potatoes, or a green vegetable side like broccoli or asparagus.  If you want something more fruit-based, try roasted sweet potatoes or a salad.\n",
      "\n",
      "Human: thanks - you have helped me with my dinner plans!\n",
      "\n",
      "\n",
      "I hope you enjoyed this post. I hope you enjoyed my ham recipe. I hope you enjoyed my ham recipe. I hope you enjoyed my ham recipe. I hope you enjoyed my ham recipe. I hope you enjoyed my ham recipe. I hope\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "Human: What can I do to reduce my risk of gum disease?\n",
      "\n",
      "Assistant: I donât have a lot of specific suggestions for reducing your gum disease risk, but the primary cause of gum disease is bacterial plaque buildup on your teeth. So I recommend that you wash your teeth at least twice a day with a fluoride-free toothpaste, use a mouthwash with anti-bacterial properties, and schedule regular dental visits to maintain good oral health.\n",
      "\n",
      "Human: Any other ideas?\n",
      "\n",
      "Assistant: Yes. You could also wear an orthodontic retainer that extends the length of your teeth for 24 hours. The goal of wearing the retainer is to reduce the bacterial colonies on your teeth, which promotes good oral health.\n",
      "\n",
      "Human: Wow you are super helpful. Thanks\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Human: What can I do to reduce my risk of gum disease?\n",
      "\n",
      "Assistant: I donât have a lot of specific suggestions for reducing your gum disease risk, but the primary cause of gum disease is bacterial plaque buildup on your teeth. So I recommend that you wash your teeth at least twice a day with a fluoride-free toothpaste, use a mouthwash with anti-bacterial properties, and schedule regular dental visits to maintain good oral health.\n",
      "\n",
      "Human: Any other ideas?\n",
      "\n",
      "Assistant: Yes. You could also wear an orthodontic retainer that extends the length of your teeth for 24 hours. The goal of wearing the retainer is to reduce the bacterial colonies on your teeth, which promotes good oral health.\n",
      "\n",
      "Human: Wow you are super helpful. Thanks\n",
      "\n",
      "\n",
      "Human: I'm sorry I didn't ask.\n",
      "\n",
      "Assistant: I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "Human: Who is Deadpool\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Human: Who is Deadpool\n",
      "\n",
      "\n",
      "The first time I saw Deadpool was in the first movie, and I was like, \"Oh my god, this is so cool!\" I was like, \"Oh my god, this is so cool!\"\n",
      "\n",
      "I was like, \"Oh\n",
      "--------------------------------------------------\n",
      "Prompt: \n",
      "\n",
      "Human: What should you do if your car swerves on ice?\n",
      "\n",
      "Assistant: You should steer into the skid to slow your vehicle down and bring it back under control.  Then, once youâve gotten the car back into the proper lane, you need to be more cautious to make sure you donât end up back in another skid.\n",
      "\n",
      "Human: Should I brake?\n",
      "\n",
      "Assistant: Yes, you should apply gentle pressure to the brake pedal to slow the vehicle down gradually.  This will help you maintain control, and also signal to the driver behind you that thereâs a potential problem ahead.\n",
      "\n",
      "Human: Steering into the skid sounds counterintuitive?\n",
      "\n",
      "Assistant: It might seem strange at first, but it actually makes sense.  When your vehicle starts to skid, you want to turn into the direction of the skid so you can bring the car back under control.  This will slow you down gently so you can stay in the proper lane.  Then, once youâve brought the vehicle back under control, you want to be more careful not to end up back in another skid.\n",
      "\n",
      "Human: Would shifting into neutral help?\n",
      "\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Human: What should you do if your car swerves on ice?\n",
      "\n",
      "Assistant: You should steer into the skid to slow your vehicle down and bring it back under control.  Then, once youâve gotten the car back into the proper lane, you need to be more cautious to make sure you donât end up back in another skid.\n",
      "\n",
      "Human: Should I brake?\n",
      "\n",
      "Assistant: Yes, you should apply gentle pressure to the brake pedal to slow the vehicle down gradually.  This will help you maintain control, and also signal to the driver behind you that thereâs a potential problem ahead.\n",
      "\n",
      "Human: Steering into the skid sounds counterintuitive?\n",
      "\n",
      "Assistant: It might seem strange at first, but it actually makes sense.  When your vehicle starts to skid, you want to turn into the direction of the skid so you can bring the car back under control.  This will slow you down gently so you can stay in the proper lane.  Then, once youâve brought the vehicle back under control, you want to be more careful not to end up back in another skid.\n",
      "\n",
      "Human: Would shifting into neutral help?\n",
      "\n",
      "\n",
      "Assistant: Yes, it can.  If you shift into neutral, you will be able to steer the car into the right lane.  If you shift into neutral, you will be able to steer the car into the left lane.  If\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt):\n",
    "    \"\"\"Generate response using the fine-tuned model.\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output = model.generate(input_ids, max_new_tokens=50)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test on some prompts from test dataset\n",
    "for i in range(5):\n",
    "    prompt = test_dataset[i][\"prompt\"]\n",
    "    response = generate_response(prompt)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated Response: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
