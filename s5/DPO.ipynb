{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangping/anaconda3/envs/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Fix missing padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS as padding token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "old_model = AutoModelForCausalLM.from_pretrained(model_name)  # Reference model (θ_old)\n",
    "\n",
    "# Resize embeddings after adding new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "old_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 76256\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 5103\n",
      "    })\n",
      "})\n",
      "{'prompt': \"\\n\\nHuman: Is it legal to have a dog that is half wolf?\\n\\nAssistant: It sounds like you’re asking whether it’s legal to have a dog that is partially wolf, as opposed to all wolf.  There’s a bit of ambiguity to your question.  What’s the context of your interest in the law?\\n\\nHuman: I'd like to get a puppy that is half wolf and half husky.\\n\\nAssistant: Ah, now I understand.  We can look up some details on the breed of wolf and husky dogs and see if they’re considered “hybrids”.\\n\\nHuman: They are called wolf dogs. It doesn't matter if you want to call them hybrids though.\\n\\nAssistant: That makes it sound like you’re seeking information to get an opinion on whether your dog is legal or not.\\n\\nHuman: Yes, does it depend on the state?\\n\\n\", 'chosen': 'Assistant: Can I ask why you’re so interested in getting a dog that is a hybrid?', 'rejected': 'Assistant: There are a lot of state laws about “hybrid” or “mutant” dogs.  Usually a dog is defined as a certain “breed” or “type” of dog, and all dogs of that breed or type are required to meet certain criteria for their physical appearance.  There are a few ways you could go about this.'}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets\n",
    "preference_data = load_dataset(\"yitingxie/rlhf-reward-datasets\")\n",
    "\n",
    "# Check available dataset splits\n",
    "print(preference_data)\n",
    "\n",
    "train_dataset = preference_data['train'].select(range(1000))\n",
    "test_dataset = preference_data['test'].select(range(10))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(prompt, response):\n",
    "    \"\"\"Tokenizes prompt + response pairs.\"\"\"\n",
    "    inputs = tokenizer(prompt, response, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs.input_ids, inputs.attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob(model, tokenizer, prompt, response):\n",
    "    \"\"\"Computes log probability of response given a prompt using the model.\"\"\"\n",
    "    inputs = tokenizer(prompt, response, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Ensure attention mask is used\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)  # Fix missing attention mask\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits[:, :-1, :]  # Ignore last token logits\n",
    "    # print(\"logits:\", logits)\n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    # print(\"log_probs:\", log_probs)\n",
    "    log_probs = torch.clamp(log_probs, min=-10, max=10)  # Prevent log(0)\n",
    "    \n",
    "    response_log_probs = log_probs.gather(2, input_ids[:, 1:].unsqueeze(-1)).squeeze(-1).sum(dim=1)\n",
    "\n",
    "    return response_log_probs.mean()  # Now works correctly with padding\n",
    "\n",
    "def dpo_loss(model, old_model, tokenizer, prompt, preferred, rejected, beta=1.0):\n",
    "    \"\"\"Computes the Direct Preference Optimization (DPO) loss.\"\"\"\n",
    "    \n",
    "    # Compute log probabilities of preferred and rejected responses\n",
    "    log_prob_w = compute_log_prob(model, tokenizer, prompt, preferred)\n",
    "    log_prob_l = compute_log_prob(model, tokenizer, prompt, rejected)\n",
    "\n",
    "    # Compute log probabilities under the reference model\n",
    "    log_prob_w_old = compute_log_prob(old_model, tokenizer, prompt, preferred)\n",
    "    log_prob_l_old = compute_log_prob(old_model, tokenizer, prompt, rejected)\n",
    "\n",
    "    # Compute winning and losing ratios\n",
    "    winning_ratio = beta * (log_prob_w - log_prob_w_old)\n",
    "    loss_ratio = beta * (log_prob_l - log_prob_l_old)\n",
    "\n",
    "    # Compute final DPO loss\n",
    "    loss = -torch.log(torch.sigmoid(winning_ratio - loss_ratio))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0: loss - 0.043321698904037476\n",
      "Epoch 0, Iteration 128: loss - 0.0017726789228618145\n",
      "Epoch 0, Iteration 256: loss - 0.0\n",
      "Epoch 0, Iteration 384: loss - 0.00018086351337842643\n",
      "Epoch 0, Iteration 512: loss - 0.0\n",
      "Epoch 0, Iteration 640: loss - 0.0010004452196881175\n",
      "Epoch 0, Iteration 768: loss - 1.2535934448242188\n",
      "Epoch 0, Iteration 896: loss - 0.03670274466276169\n",
      "Epoch 1, Loss: 0.8186\n",
      "Epoch 1, Iteration 0: loss - 5.960467319710006e-08\n",
      "Epoch 1, Iteration 128: loss - 0.01660657487809658\n",
      "Epoch 1, Iteration 256: loss - 0.11035837233066559\n",
      "Epoch 1, Iteration 384: loss - 0.006150395609438419\n",
      "Epoch 1, Iteration 512: loss - 0.0\n",
      "Epoch 1, Iteration 640: loss - 0.00031707261223345995\n",
      "Epoch 1, Iteration 768: loss - 1.1229934692382812\n",
      "Epoch 1, Iteration 896: loss - 0.017250964418053627\n",
      "Epoch 2, Loss: 0.4754\n",
      "Epoch 2, Iteration 0: loss - 2.980239344196889e-07\n",
      "Epoch 2, Iteration 128: loss - 6.162431964185089e-05\n",
      "Epoch 2, Iteration 256: loss - 0.0\n",
      "Epoch 2, Iteration 384: loss - 0.00018730112060438842\n",
      "Epoch 2, Iteration 512: loss - 2.980232949312267e-08\n",
      "Epoch 2, Iteration 640: loss - 7.89403056842275e-05\n",
      "Epoch 2, Iteration 768: loss - 0.2197505086660385\n",
      "Epoch 2, Iteration 896: loss - 0.00800240132957697\n",
      "Epoch 3, Loss: 0.3239\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Define gradient accumulation steps\n",
    "gradient_accumulation_steps = 16  # Adjust to control memory usage\n",
    "\n",
    "num_epochs = 3  # Increase for better results\n",
    "batch_size = 4  # Mini-batch size before accumulation\n",
    "print_interval = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, len(train_dataset), batch_size):\n",
    "        \n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        optimizer.zero_grad() if (i % (batch_size * gradient_accumulation_steps)) == 0 else None\n",
    "\n",
    "        # Compute DPO loss\n",
    "        loss = dpo_loss(model, old_model, tokenizer, batch[\"prompt\"], batch[\"chosen\"], batch[\"rejected\"])\n",
    "        loss = loss.mean()  # Ensure loss is a scalar\n",
    "\n",
    "        # Normalize loss by accumulation steps\n",
    "        loss = loss / gradient_accumulation_steps  \n",
    "        loss.backward()  # Accumulate gradients\n",
    "\n",
    "        # Perform optimizer step after accumulation steps\n",
    "        if (i // batch_size + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()  # Update model weights\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        # Print loss occasionally\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch {epoch}, Iteration {i}: loss - {loss.item()}\")\n",
    "\n",
    "        total_loss += loss.item() * gradient_accumulation_steps  # Rescale loss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "\n",
      "Human: Do you know why turkeys became the official food of thanksgiving?\n",
      "\n",
      "\n",
      "Chosen: Assistant: To be honest, I don’t know anything about that. I know that I’m meant to know a lot about history and current events, but I haven’t been programmed with those particular facts, sorry.\n",
      "Generated Response: \n",
      "\n",
      "Human: Do you know why turkeys became the official food of thanksgiving?\n",
      "\n",
      "\n",
      "Turkeys: Because they were the first to eat the food of thanksgiving. They were the first to eat the food of thanksgiving. They were\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt):\n",
    "    \"\"\"Generates a response from the fine-tuned model.\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output = model.generate(input_ids, max_length=50)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the fine-tuned model\n",
    "prompt = test_dataset[0][\"prompt\"]\n",
    "preferred = test_dataset[0][\"chosen\"]\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Chosen:\", preferred)\n",
    "print(\"Generated Response:\", generate_response(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
