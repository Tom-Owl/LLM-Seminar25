{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import RobertaTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_metrics(p: transformers.EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"eval_accuracy\": accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, input_list, labels, tokenizer):\n",
    "        self.encoded_data = tokenizer(input_list, \n",
    "                                      truncation=True, \n",
    "                                      padding=True, \n",
    "                                      return_tensors=\"pt\", \n",
    "                                      max_length=256)\n",
    "       \n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"input_ids\": self.encoded_data['input_ids'][idx],\n",
    "            \"attention_mask\": self.encoded_data['attention_mask'][idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class FineTuner:\n",
    "    def __init__(self, \n",
    "                 model_name, \n",
    "                 output_dir,\n",
    "                 logging_dir):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(model_name, output_attentions=False).to(self.device)\n",
    "        self.training_args = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.output_dir = output_dir\n",
    "        self.logging_dir = logging_dir\n",
    "\n",
    "    # Usage within some class or function:\n",
    "    def prepare_data(self, train_input_list, train_label, val_input_list, val_label):\n",
    "        self.train_dataset = SentimentDataset(train_input_list, train_label, self.tokenizer)\n",
    "        self.val_dataset = SentimentDataset(val_input_list, val_label, self.tokenizer)\n",
    "\n",
    "    def setup_training(self, batch_size=64, num_epochs=5):\n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            logging_dir=self.logging_dir,\n",
    "            logging_steps=1000,  # Log every 100 steps\n",
    "            evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=500, \n",
    "            save_strategy=\"epoch\",  # Do not save any models except the best one\n",
    "            load_best_model_at_end=True,  # Load the best model in terms of evaluation metric\n",
    "            metric_for_best_model=\"eval_accuracy\",  # Define the desired metric here\n",
    "            greater_is_better=True,\n",
    "            gradient_accumulation_steps=4,\n",
    "            max_grad_norm=1.0,\n",
    "        )\n",
    "            \n",
    "    def fine_tune_all(self):\n",
    "        trainer = Trainer(\n",
    "            model=self.base_model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "            compute_metrics=compute_metrics,  # Add this line\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/lei/.cache/huggingface/modules/datasets_modules/datasets/imdb/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0 (last modified on Sun Jun 11 15:07:30 2023) since it couldn't be found locally at imdb, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Prepare 100 samples for training and 20 samples for testing\n",
    "train_data = dataset[\"train\"].shuffle(seed=42).select(range(100))\n",
    "val_data = dataset[\"train\"].shuffle(seed=42).select(range(10))\n",
    "test_data = dataset[\"test\"].shuffle(seed=42).select(range(20))\n",
    "\n",
    "# Organize as train_x (sentences) and train_y (labels)\n",
    "train_x = train_data[\"text\"]\n",
    "train_y = train_data[\"label\"]\n",
    "\n",
    "val_x = val_data[\"text\"]\n",
    "val_y = val_data[\"label\"]\n",
    "\n",
    "# Organize as test_x (sentences) and test_y (labels)\n",
    "test_x = test_data[\"text\"]\n",
    "test_y = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lei/anaconda3/envs/RLF/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/5 00:57 < 01:55, 0.02 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702061</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.701827</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.701158</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tuner = FineTuner(model_name=\"siebert/sentiment-roberta-large-english\",\n",
    "                    output_dir = '../results/roberta',\n",
    "                    logging_dir = '../logs/roberta')\n",
    "\n",
    "tuner.prepare_data(train_x, train_y, val_x, val_y)\n",
    "tuner.setup_training(batch_size=64)\n",
    "tuner.fine_tune_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
